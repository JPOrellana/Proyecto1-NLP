{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce25c34",
   "metadata": {},
   "source": [
    "# Proyecto 1 NLP – Clasificación supervisada\n",
    "\n",
    "**Universidad del Valle de Guatemala**  \n",
    "**Facultad de Ingeniería**  \n",
    "**Departamento de Ciencias de la Computación**  \n",
    "**Procesamiento de Lenguaje Natural**   \n",
    "\n",
    "## Integrantes: \n",
    "- Pablo Orellana\n",
    "- Diego Leiva\n",
    "- Renatto Guzmán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed535ae",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack, coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8013c",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento del Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fddc4f",
   "metadata": {},
   "source": [
    "### Cargar Corpus\n",
    "\n",
    "Corpus obtenido de Kaggle en: [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/luisdiegofv97/imdb-dataset-of-50k-movie-reviews-spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/IMDB Dataset SPANISH.csv')\n",
    "print(f\"=== Dataset ===\")\n",
    "print(f\"Documentos: {df.shape[0]}\")\n",
    "print(f\"Características: {df.shape[1]}\\n\")\n",
    "print(\"=== Información del Dataset ===\")\n",
    "df.info()\n",
    "print(\"\\n=== Primeras 5 filas del Dataset ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683ebcb",
   "metadata": {},
   "source": [
    "### Procesamiento de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas en ingles o innecesarias\n",
    "df = df.drop(columns=['review_en', 'Unnamed: 0', 'sentiment'])\n",
    "\n",
    "# Renombrar columnas\n",
    "df = df.rename(columns={'review_es': 'review'})\n",
    "\n",
    "# Convertir variable objetivo a binaria\n",
    "df['sentimiento'] = df['sentimiento'].map({'positivo': 1, 'negativo': 0})\n",
    "\n",
    "# Eliminar duplicados\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"=== Dataset Procesado ===\")\n",
    "print(f\"Documentos: {df.shape[0]}\")\n",
    "print(f\"Características: {df.shape[1]}\\n\")\n",
    "print(\"\\n=== Primeras 5 filas del Dataset ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c99ad7",
   "metadata": {},
   "source": [
    "### Normalización de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e435140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Función para limpiar y normalizar texto en español.\n",
    "    Parámetros:\n",
    "        texto (str): Texto a limpiar.\n",
    "    Retorna:\n",
    "        str: Texto limpio y normalizado.\n",
    "    \"\"\"\n",
    "    # 1) a minúsculas\n",
    "    s = str(texto).lower().strip()\n",
    "    # 2) quitar tildes/diacríticos\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    # 3) remover URLs, emails, @usuarios, #hashtags\n",
    "    s = re.sub(r'https?://\\S+|www\\.\\S+', ' ', s)\n",
    "    s = re.sub(r'\\S+@\\S+\\.\\S+', ' ', s)\n",
    "    s = re.sub(r'[@#]\\w+', ' ', s)\n",
    "    # 4) quitar puntuación y símbolos\n",
    "    s = re.sub(r'[^\\w\\s]', ' ', s, flags=re.UNICODE)  # elimina ¡¿, comas, puntos, comillas, etc.\n",
    "    s = re.sub(r'_', ' ', s)\n",
    "    # 5) espacios múltiples a uno\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    # 6) eliminar numeros\n",
    "    s = re.sub(r'\\d+', ' ', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columna normalizada\n",
    "df[\"review_norm\"] = df[\"review\"].apply(limpiar_texto)\n",
    "\n",
    "# Mostrar ejemplos de limpieza\n",
    "print(\"Ejemplo (antes y después):\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"- Original: {df['review'].iloc[i][:50]}...\")\n",
    "    print(f\"- Limpia  : {df['review_norm'].iloc[i][:50]}...\\n\")\n",
    "\n",
    "# Verificar si hay vacios después de la limpieza\n",
    "vacios = df['review_norm'].str.strip().eq('')\n",
    "print(f\"Número de filas con texto vacío después de la limpieza: {vacios.sum()}\")\n",
    "# Eliminar filas con texto vacío\n",
    "df = df[~vacios]\n",
    "print(f\"Filas restantes después de eliminar textos vacíos: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ef349",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "Se amplian las stopwords de NTLK con el conjunto de stopwords de [alir3z4 Stop Words](https://alir3z4.github.io/stop-words/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza una cadena de texto, convirtiéndola a minúsculas, eliminando tildes y espacios en blanco.\n",
    "    Parámetros:\n",
    "        s (str): Cadena de texto a normalizar.\n",
    "    Retorna:\n",
    "        str: Cadena de texto normalizada.\n",
    "    \"\"\"\n",
    "    s = s.lower().strip()\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def norm_set(words):\n",
    "    \"\"\"\n",
    "    Normaliza un conjunto de palabras.\n",
    "    Parámetros:\n",
    "        words (iterable): Conjunto de palabras a normalizar.\n",
    "    Retorna:\n",
    "        set: Conjunto de palabras normalizadas.\n",
    "    \"\"\"\n",
    "    return {normalize(w) for w in words if w.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar stopwords de NLTK si no se han descargado\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# 1) base y extras\n",
    "sw_es = set(stopwords.words('spanish'))\n",
    "sw_en = set(stopwords.words('english'))\n",
    "\n",
    "# Agregar stopwords comunes en español e inglés extra\n",
    "with open(\"Data/spanish_stopwords.txt\", encoding=\"utf-8\") as f:\n",
    "    es_extra = {ln.strip() for ln in f if ln.strip()}\n",
    "with open(\"Data/english_stopwords.txt\", encoding=\"utf-8\") as f:\n",
    "    en_extra = {ln.strip() for ln in f if ln.strip()}\n",
    "\n",
    "# 2) protegidas\n",
    "negaciones = {\n",
    "    \"no\",\"nunca\",\"jamas\",\"ni\",\"sin\",\"tampoco\",\"ningun\",\n",
    "    \"ninguna\",\"ninguno\",\"ningunas\",\"ningunos\",\"nadie\",\"nada\"\n",
    "    }\n",
    "contrastivos = {\n",
    "    \"pero\",\"aunque\",\"sin\",\"embargo\",\"sinembargo\",\"no\",\n",
    "    \"obstante\",\"sino\",\"excepto\",\"salvo\",\"aunquesi\"\n",
    "    }\n",
    "intensif = {\"muy\",\"tan\",\"tanto\",\"tantos\",\"tantas\",\n",
    "            \"demasiado\",\"demasiada\",\"demasiados\",\n",
    "            \"demasiadas\",\"super\",\"re\",\"hiper\",\"bastante\",\n",
    "            \"apenas\",\"poco\",\"poca\",\"pocos\",\"pocas\",\"casi\",\n",
    "            \"algo\",\"sumamente\"\n",
    "            }\n",
    "\n",
    "protegidas = norm_set(negaciones | contrastivos | intensif)\n",
    "\n",
    "# 3) normalizar todo\n",
    "sw_total_norm = norm_set(sw_es|es_extra|sw_en|en_extra) - protegidas\n",
    "print(\"=== Resumen de stopwords ===\")\n",
    "print(f\"Stopwords español base: {len(sw_es)} | extra: {len(es_extra)}\")\n",
    "print(f\"Stopwords inglés base: {len(sw_en)} | extra: {len(en_extra)}\")\n",
    "print(f\"Stopwords finales: {len(sw_total_norm)}\")\n",
    "\n",
    "# 4) regex con bordes de palabra\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(sorted(map(re.escape, sw_total_norm), key=len, reverse=True)) + r')\\b')\n",
    "\n",
    "# 5) aplicar a la columna ya normalizada\n",
    "df[\"review_sin_sw\"] = (\n",
    "    df[\"review_norm\"]\n",
    "      .str.replace(pattern, \" \", regex=True)\n",
    "      .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "# Chequeo rápido\n",
    "print(\"\\nEjemplos (antes y sin stopwords):\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"- Reseña        : {df['review_norm'].iloc[i][:50]}...\")\n",
    "    print(f\"- Sin Stopwords : {df['review_sin_sw'].iloc[i][:50]}...\\n\")\n",
    "\n",
    "# Verificar si hay vacios después de remover stopwords\n",
    "vacios_sw = df['review_sin_sw'].str.strip().eq('')\n",
    "print(f\"Número de filas con texto vacío después de remover stopwords: {vacios_sw.sum()}\")\n",
    "# Eliminar filas con texto vacío\n",
    "df = df[~vacios_sw]\n",
    "print(f\"Filas restantes después de eliminar textos vacíos: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc4e3f",
   "metadata": {},
   "source": [
    "### Tokenización y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar recursos de NLTK si no se han descargado\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "# Aplicar stemming a la columna sin stopwords\n",
    "# tokens crudos (sin stopwords)\n",
    "df[\"tokens\"] = df[\"review_sin_sw\"].apply(lambda x: word_tokenize(x, language=\"spanish\"))\n",
    "\n",
    "# tokens stem (lista)\n",
    "df[\"tokens_stem\"] = df[\"tokens\"].apply(lambda xs: [stemmer.stem(t) for t in xs])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da390013",
   "metadata": {},
   "source": [
    "### Levenshtein\n",
    "No se aplica a todo el dataset debido a que:\n",
    "- El corpus tiene ~50 000 reseñas → millones de palabras únicas.\n",
    "- Comparar cada palabra con todas las demás sería $(O(n^2))$, ineficiente y sin valor práctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3037c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Distancia de edición mínima entre a y b.\n",
    "    \n",
    "    Parámetros:\n",
    "        a (str): Primera cadena.\n",
    "        b (str): Segunda cadena.\n",
    "    Retorna:\n",
    "        int: Distancia de edición mínima.\n",
    "        \"\"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "    if not a:\n",
    "        return len(b)\n",
    "    if not b:\n",
    "        return len(a)\n",
    "    dp = np.zeros((len(a)+1, len(b)+1), dtype=int)\n",
    "    dp[0, :] = np.arange(len(b)+1)\n",
    "    dp[:, 0] = np.arange(len(a)+1)\n",
    "    for i in range(1, len(a)+1):\n",
    "        for j in range(1, len(b)+1):\n",
    "            cost = 0 if a[i-1] == b[j-1] else 1\n",
    "            dp[i, j] = min(dp[i-1, j]+1, dp[i, j-1]+1, dp[i-1, j-1]+cost)\n",
    "    return dp[len(a), len(b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e233a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción del vocabulario\n",
    "# Tomamos los tokens sin stem para detectar errores reales\n",
    "vocab_counts = Counter(t for toks in df[\"tokens\"] for t in toks)\n",
    "print(f\"Vocabulario total: {len(vocab_counts)} palabras\")\n",
    "\n",
    "# Palabras frecuentes (consideradas válidas)\n",
    "freq_words = {w for w, c in vocab_counts.items() if c >= 5}\n",
    "\n",
    "# Ejemplo de corrección\n",
    "palabra = \"exelente\"\n",
    "distancias = {w: levenshtein(palabra, w) for w in list(freq_words)[:2000]}  # comparar con un subconjunto\n",
    "sugerencia = min(distancias, key=distancias.get)\n",
    "print(f\"Palabra: {palabra} → Sugerencia más cercana: {sugerencia}\")\n",
    "\n",
    "# Opcional: aplicar a todo el vocabulario raro (<3 apariciones)\n",
    "rare = {w for w, c in vocab_counts.items() if c < 3}\n",
    "correcciones = {}\n",
    "for w in list(rare)[:20]:\n",
    "    candidatos = {v: levenshtein(w, v) for v in freq_words if abs(len(v)-len(w)) <= 2}\n",
    "    if candidatos:\n",
    "        best = min(candidatos, key=candidatos.get)\n",
    "        if candidatos[best] <= 1:\n",
    "            correcciones[w] = best\n",
    "\n",
    "print(\"\\nEjemplos de correcciones detectadas:\")\n",
    "for k, v in list(correcciones.items())[:10]:\n",
    "    print(f\"{k} → {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d7e21",
   "metadata": {},
   "source": [
    "## 2. Representación del Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2c212",
   "metadata": {},
   "source": [
    "### Bolsa de Palabras (BoW) y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f38578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear reseñas con \"stems\" para vectorizadores basados en conteo\n",
    "df[\"text_stem\"] = df[\"tokens_stem\"].apply(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Aplicar BoW y TF-IDF\n",
    "bow_vec = CountVectorizer(min_df=5, max_df=0.5, ngram_range=(1,2))   # unigrams+bigramas\n",
    "tfidf_vec = TfidfVectorizer(min_df=5, max_df=0.5, ngram_range=(1,2), sublinear_tf=True)\n",
    "\n",
    "# Transformar\n",
    "X_bow   = bow_vec.fit_transform(df[\"text_stem\"])\n",
    "X_tfidf = tfidf_vec.fit_transform(df[\"text_stem\"])\n",
    "\n",
    "print(\"=== BoW ===\")\n",
    "print(f\"Documentos: {X_bow.shape[0]}\")\n",
    "print(f\"Términos distintos: {X_bow.shape[1]}\")\n",
    "print(f\"Densidad promedio: {X_bow.nnz / (X_bow.shape[0]*X_bow.shape[1]):.6f}\")\n",
    "print(f\"Entradas no nulas (nnz): {X_bow.nnz:,}\")\n",
    "print(f\"Promedio de términos no nulos por documento: {X_bow.nnz / X_bow.shape[0]:.1f}\\n\")\n",
    "\n",
    "print(\"=== TF-IDF ===\")\n",
    "print(f\"Documentos: {X_tfidf.shape[0]}\")\n",
    "print(f\"Términos distintos: {X_tfidf.shape[1]}\")\n",
    "print(f\"Densidad promedio: {X_tfidf.nnz / (X_tfidf.shape[0]*X_tfidf.shape[1]):.6f}\")\n",
    "print(f\"Entradas no nulas (nnz): {X_tfidf.nnz:,}\")\n",
    "print(f\"Promedio de términos no nulos por documento: {X_tfidf.nnz / X_tfidf.shape[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad317ffa",
   "metadata": {},
   "source": [
    "### Matriz de co-ocurrencia y aplicación de PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitar a vocabulario top-V para controlar memoria\n",
    "V = 10000  # Solo 10k palabras más frecuentes\n",
    "# Obtener mas frecuentes\n",
    "freq = Counter(t for toks in df[\"tokens\"] for t in toks).most_common(V)\n",
    "itos = [w for w,_ in freq]\n",
    "stoi = {w:i for i,w in enumerate(itos)}\n",
    "\n",
    "# Contar co-ocurrencias en ventana\n",
    "window = 4     # contexto a cada lado\n",
    "pair_counts = defaultdict(int)\n",
    "\n",
    "# Iterar sobre tokens y contar pares\n",
    "for toks in df[\"tokens\"]:\n",
    "    # obtener índices de vocabulario\n",
    "    idxs = [stoi[t] for t in toks if t in stoi]\n",
    "    # contar pares en ventana\n",
    "    for i, wi in enumerate(idxs):\n",
    "        j0 = max(0, i-window) # inicio de ventana\n",
    "        j1 = min(len(idxs), i+window+1) # fin de ventana\n",
    "        # contar pares\n",
    "        for j in range(j0, j1):\n",
    "            if j == i: \n",
    "                continue\n",
    "            wj = idxs[j]\n",
    "            if wi == wj: \n",
    "                continue\n",
    "            # conteo simétrico\n",
    "            a,b = (wi,wj) if wi < wj else (wj,wi)\n",
    "            pair_counts[(a,b)] += 1 \n",
    "\n",
    "# Construir matriz simétrica COO\n",
    "rows, cols, data = [], [], [] # listas para COO\n",
    "\n",
    "# llenar listas\n",
    "for (i,j), c in pair_counts.items():\n",
    "    rows += [i, j]\n",
    "    cols += [j, i]\n",
    "    data += [c, c]\n",
    "# Crear matriz dispersa COO\n",
    "C = coo_matrix((data, (rows, cols)), shape=(V, V), dtype=np.float64).tocsr()\n",
    "total = C.sum() # suma total de co-ocurrencias\n",
    "\n",
    "# Calcular PPMI\n",
    "# PMI(i,j) = log2( P(i,j) / (P(i)P(j)) ), PPMI = max(PMI, 0)\n",
    "row_sums = np.asarray(C.sum(axis=1)).ravel()\n",
    "col_sums = row_sums  # simétrica\n",
    "C_coo = C.tocoo()\n",
    "p_ij = C_coo.data / total\n",
    "p_i  = row_sums[C_coo.row] / total\n",
    "p_j  = col_sums[C_coo.col] / total\n",
    "pmi  = np.log2((p_ij / (p_i * p_j)) + 1e-12)\n",
    "ppmi_data = np.maximum(pmi, 0.0)\n",
    "\n",
    "# Crear matriz PPMI dispersa\n",
    "PPMI = coo_matrix((ppmi_data, (C_coo.row, C_coo.col)), shape=C.shape).tocsr()\n",
    "print(\"=== MATRIZ DE CO-OCURRENCIA ===\")\n",
    "print(f\"Dimensión: {C.shape}\")\n",
    "print(f\"Entradas no nulas (nnz): {C.nnz:,}\")\n",
    "print(f\"Densidad: {C.nnz / (C.shape[0]*C.shape[1]):.8f}\")\n",
    "print(f\"Total de co-ocurrencias contadas: {int(total):,}\")\n",
    "print(f\"Media de conteo por par no nulo: {C.data.mean():.2f}\")\n",
    "\n",
    "print(\"\\n=== MATRIZ PPMI ===\")\n",
    "print(f\"Dimensión: {PPMI.shape}\")\n",
    "print(f\"Entradas no nulas (nnz): {PPMI.nnz:,}\")\n",
    "print(f\"Densidad: {PPMI.nnz / (PPMI.shape[0]*PPMI.shape[1]):.8f}\")\n",
    "print(f\"Valor medio PPMI (>0): {PPMI.data.mean():.4f}\")\n",
    "print(f\"Valor máximo PPMI: {PPMI.data.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c148a",
   "metadata": {},
   "source": [
    "### Implementación de embeddings con Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para Word2Vec/FastText\n",
    "sentences = df[\"tokens\"].tolist()\n",
    "\n",
    "# Word2Vec con skip-gram\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=5, \n",
    "    workers=4, \n",
    "    sg=1, \n",
    "    negative=10, \n",
    "    epochs=5\n",
    "    )\n",
    "\n",
    "# Vectores de documento por promedio de palabras presentes\n",
    "def doc_vector(tokens, kv):\n",
    "    vecs = [kv[w] for w in tokens if w in kv]\n",
    "    if not vecs:\n",
    "        return np.zeros(kv.vector_size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "doc_vecs_w2v = np.vstack([doc_vector(toks, w2v.wv) for toks in sentences])\n",
    "palabras_ejemplo = [\"bueno\", \"malo\", \"excelente\", \"terrible\", \"divertido\", \"aburrido\"]\n",
    "\n",
    "# Mostrar dimendiones y ejemplos\n",
    "print(\"=== Word2Vec ===\")\n",
    "print(f\"Tamaño del vocabulario w2v: {len(w2v.wv)}\")\n",
    "print(f\"Dimensiones de los vectores w2v: {w2v.wv.vectors.shape[1]}\")\n",
    "print(\"Ejemplos: \")\n",
    "for w in palabras_ejemplo:\n",
    "    if w in w2v.wv:\n",
    "        print(f\"Similares a '{w}':\", [p for p,_ in w2v.wv.most_similar(w)[:5]])\n",
    "    else:\n",
    "        print(f\"{w}: No en vocabulario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84abd86",
   "metadata": {},
   "source": [
    "### Comparación mediante PCA o t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X: np.ndarray, words: list, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Visualiza vectores en 2D usando PCA.\n",
    "    Parámetros:\n",
    "        X: Matriz de vectores (n_samples, n_features)\n",
    "        words: Lista de palabras correspondientes a los vectores\n",
    "        title: Título del gráfico\n",
    "    Retorna: \n",
    "        None\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    X2 = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X2[:,0], X2[:,1], s=8)\n",
    "    for i,w in enumerate(words[:60]):  # anota pocas\n",
    "        plt.annotate(w, (X2[i,0], X2[i,1]), fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_tsne(X: np.ndarray, words: list, title: str, n: int = 200) -> None:\n",
    "    \"\"\"\n",
    "    Visualiza vectores en 2D usando t-SNE.\n",
    "    Parámetros:\n",
    "        X: Matriz de vectores (n_samples, n_features)\n",
    "        words: Lista de palabras correspondientes a los vectores\n",
    "        title: Título del gráfico\n",
    "        n: Número de muestras a visualizar\n",
    "    Retorna: \n",
    "        None\n",
    "    \"\"\"\n",
    "    Xs = X[:n]; ws = words[:n]\n",
    "    tsne = TSNE(n_components=2, init=\"random\", perplexity=30,\n",
    "                learning_rate=\"auto\", max_iter=1000, random_state=0)\n",
    "    X2 = tsne.fit_transform(Xs)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X2[:,0], X2[:,1], s=8)\n",
    "    for i,w in enumerate(ws[:50]):\n",
    "        plt.annotate(w, (X2[i,0], X2[i,1]), fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Selección de palabras comunes para comparar W2V vs FastText ===\n",
    "K = 300  # muestra\n",
    "cand = [w for w,_ in freq]  # freq ya calculado antes\n",
    "common = [w for w in cand if (w in w2v.wv.key_to_index) and (w in ft.wv.key_to_index)]\n",
    "words = common[:K]\n",
    "print(f\"Palabras comunes usadas: {len(words)}\")\n",
    "\n",
    "# === Matrices de vectores ===\n",
    "X_w2v = np.vstack([w2v.wv[w] for w in words])\n",
    "\n",
    "# === W2V: PCA y t-SNE ===\n",
    "plot_pca(X_w2v, words, \"W2V palabras – PCA (muestra común)\")\n",
    "plot_tsne(X_w2v, words, \"W2V palabras – t-SNE (muestra común)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
